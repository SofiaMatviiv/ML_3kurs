{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9ba6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2577a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    \n",
    "    def __init__(self, layers_sizes, normalize = True, learning_rate = 0.01, num_iter = 15000, costs_iters=300, tol=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iter = num_iter\n",
    "        self.normalize = normalize\n",
    "        self.layers_sizes = layers_sizes\n",
    "        self.tol = tol\n",
    "        self.layers = len(layers_sizes) + 1\n",
    "        self.costs_iters = costs_iters\n",
    "    \n",
    "    def __normalize(self, X, mean = None, std = None):\n",
    "        n = X.shape[0]\n",
    "        m = mean\n",
    "        if m is None:\n",
    "            m = np.mean(X, axis=1).reshape((n, 1))\n",
    "        s = std\n",
    "        if s is None:\n",
    "            s = np.std(X, axis=1).reshape((n, 1))\n",
    "        X_new = (X - m) / s\n",
    "        return X_new, m, s\n",
    "    \n",
    "    def __softmax(self, Z):\n",
    "        e = np.exp(Z)\n",
    "        return e/np.sum(e, axis=0, keepdims=True)\n",
    "    \n",
    "    def __relu(self, Z):\n",
    "        return np.maximum(0,Z)\n",
    "    \n",
    "    def __relu_derivative(self, A):\n",
    "        return np.greater_equal(A, 0).astype(int)\n",
    "    \n",
    "    def __initialize_parameters(self):\n",
    "        W, b = [], []\n",
    "        \n",
    "        for l in range(1,self.layers+1):\n",
    "            W_l = np.random.randn(self.layers_sizes[l], self.layers_sizes[l-1]) * 0.01\n",
    "            b_l = np.zeros((self.layers_sizes[l], 1))\n",
    "            W.append(W_l)\n",
    "            b.append(b_l)\n",
    "        \n",
    "        self.parameters = {\"W\" : W, \"b\" : b}\n",
    "        \n",
    "        for key in self.parameters:\n",
    "            for key_i,i in zip(self.parameters[key],range(len(self.parameters[key]))):\n",
    "                print(\"{}{}: {}\".format(key,i+1,key_i.shape))\n",
    "       \n",
    "    def __forward_propagation(self, X):            \n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        Z, A = [], [] \n",
    "\n",
    "        A.append(X)\n",
    "        for l in range(0,self.layers-1):\n",
    "            Z_l = np.dot(W[l],A[l]) + b[l]\n",
    "            A_l = self.__relu(Z_l)\n",
    "            Z.append(Z_l)\n",
    "            A.append(A_l)\n",
    "        \n",
    "        l = self.layers-1\n",
    "        Z_l = np.dot(W[l],A[l]) + b[l]\n",
    "        A_l = self.__softmax(Z_l)\n",
    "        Z.append(Z_l)\n",
    "        A.append(A_l)\n",
    "        \n",
    "        cache = (Z,A)\n",
    "        return A[-1], cache\n",
    "    \n",
    "    def compute_cost(self, A, Y):\n",
    "        m = Y.shape[1]\n",
    "        res = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "        J = -(1 / m) * np.sum(res)\n",
    "        return J\n",
    "    \n",
    "    def __backward_propagation(self, X, Y, cache):\n",
    "        m = X.shape[1]\n",
    "        \n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        (Z,A) = cache\n",
    "        \n",
    "        dZ, dW, db = [], [], []\n",
    "        \n",
    "        dZ_l = A[-1] - Y\n",
    "        dW_l = 1. / m * np.dot(dZ_l, A[-2].T)\n",
    "        db_l = 1. / m * np.sum(dZ_l, axis = 1, keepdims = True)\n",
    "        \n",
    "        dZ.insert(0,dZ_l)\n",
    "        dW.insert(0,dW_l)\n",
    "        db.insert(0,db_l)\n",
    "        \n",
    "        for l in range(self.layers-1,0,-1):\n",
    "            dA_l = np.dot(W[l].T,dZ_l)\n",
    "            dZ_l = np.multiply(dA_l, self.__relu_derivative(A[l]))\n",
    "            dW_l = 1. / m * np.dot(dZ_l, A[l-1].T)\n",
    "            db_l = 1. / m * np.sum(dZ_l, axis = 1, keepdims = True)\n",
    "            dZ.insert(0,dZ_l)\n",
    "            dW.insert(0,dW_l)\n",
    "            db.insert(0,db_l)\n",
    "        \n",
    "        grads = {\"dZ\":dZ,\"dW\":dW,\"db\":db}\n",
    "        return grads\n",
    "  \n",
    "    \n",
    "    def __update_parameters(self, grads):        \n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        dW = grads[\"dW\"]\n",
    "        db = grads[\"db\"]\n",
    "    \n",
    "        for l in range(0,len(W)):\n",
    "            W[l] = W[l] - self.learning_rate * dW[l]\n",
    "            b[l] = b[l] - self.learning_rate * db[l]\n",
    "            \n",
    "        self.parameters[\"W\"] = W\n",
    "        self.parameters[\"b\"] = b\n",
    "\n",
    "        \n",
    "    def fit(self, X_vert, Y_vert, print_cost = True):\n",
    "        \n",
    "        X, Y = X_vert.T, Y_vert.T\n",
    "        Y = Y.reshape((Y.shape[0], 1))\n",
    "        enc = OneHotEncoder(handle_unknown='ignore')\n",
    "        self.enc = enc\n",
    "        Y = enc.fit_transform(Y).toarray().astype(int).T\n",
    "        \n",
    "        if self.normalize:\n",
    "            X, self.__mean, self.__std = self.__normalize(X)\n",
    "        \n",
    "        costs = []\n",
    "        costs_imp = []\n",
    "        \n",
    "        m = X.shape[1]\n",
    "        n_x = X.shape[0]\n",
    "        C = Y.shape[0]\n",
    "        \n",
    "        self.layers_sizes.insert(0,n_x)\n",
    "        self.layers_sizes.append(C);\n",
    "        \n",
    "        self.__initialize_parameters()\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            A, cache = self.__forward_propagation(X)\n",
    "\n",
    "            cost = self.compute_cost(A, Y)\n",
    "            \n",
    "            grads = self.__backward_propagation(X, Y, cache)\n",
    "\n",
    "            self.__update_parameters(grads)\n",
    "\n",
    "            if print_cost and i % 1000 == 0:\n",
    "                print(\"{}-th iteration: {}\".format(i, cost))\n",
    "\n",
    "            if i % 1000 == 0:\n",
    "                costs.append(cost)\n",
    "            \n",
    "            costs_imp.append(cost)\n",
    "            if i > self.costs_iters:\n",
    "                len_imp = len(costs_imp)-1\n",
    "                if abs(costs_imp[len_imp] - costs_imp[len_imp-self.costs_iters]) < self.tol:\n",
    "                    print(\"Stop iteration: {}\".format(i))\n",
    "                    break;\n",
    "\n",
    "        if print_cost:\n",
    "            plt.plot(costs)\n",
    "            plt.ylabel(\"Cost\")\n",
    "            plt.xlabel(\"Iteration, *1000\")\n",
    "            plt.show()\n",
    "    \n",
    "    def predict_proba(self, X_vert):\n",
    "        X = X_vert.T\n",
    "        if self.normalize:\n",
    "            X, _, _ = self.__normalize(X, self.__mean, self.__std)\n",
    "        \n",
    "        probs = self.__forward_propagation(X)[0].T\n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X_vert):\n",
    "        positive_probs = self.predict_proba(X_vert)\n",
    "        max_cols = positive_probs.max(axis=1).reshape((-1,1))\n",
    "        pred = (positive_probs == max_cols).astype(int)\n",
    "        pred = self.enc.inverse_transform(pred)\n",
    "        pred = pred.reshape((-1,)).T\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac3f88b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "X, Y = load_iris(return_X_y = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d3b3b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "cls = MLPClassifier(hidden_layer_sizes = (20,), max_iter = 10000,  solver = 'adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d740fbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20,), max_iter=10000)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2d90488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.94816668e-01, 5.18333190e-03, 1.81558657e-11],\n",
       "       [9.86417498e-01, 1.35825012e-02, 3.04844803e-10],\n",
       "       [9.91840080e-01, 8.15991962e-03, 1.08001932e-10],\n",
       "       [9.85078002e-01, 1.49219979e-02, 5.90771453e-10],\n",
       "       [9.95299464e-01, 4.70053597e-03, 1.56830124e-11],\n",
       "       [9.93107367e-01, 6.89263329e-03, 2.31303688e-11],\n",
       "       [9.91052341e-01, 8.94765932e-03, 1.54881766e-10],\n",
       "       [9.92423559e-01, 7.57644053e-03, 5.62439925e-11],\n",
       "       [9.81337890e-01, 1.86621083e-02, 1.44349963e-09],\n",
       "       [9.89481312e-01, 1.05186881e-02, 1.55663804e-10],\n",
       "       [9.96000122e-01, 3.99987759e-03, 5.90239803e-12],\n",
       "       [9.90159916e-01, 9.84008354e-03, 1.46111434e-10],\n",
       "       [9.89275573e-01, 1.07244264e-02, 1.90474211e-10],\n",
       "       [9.90151910e-01, 9.84809001e-03, 2.57641641e-10],\n",
       "       [9.98302641e-01, 1.69735923e-03, 2.89725910e-13],\n",
       "       [9.97918070e-01, 2.08192977e-03, 5.47342165e-13],\n",
       "       [9.96594629e-01, 3.40537072e-03, 3.65437588e-12],\n",
       "       [9.93505288e-01, 6.49471172e-03, 3.26207396e-11],\n",
       "       [9.94121007e-01, 5.87899345e-03, 1.00641606e-11],\n",
       "       [9.95414562e-01, 4.58543778e-03, 1.28572469e-11],\n",
       "       [9.89582652e-01, 1.04173479e-02, 7.02774276e-11],\n",
       "       [9.92820605e-01, 7.17939519e-03, 4.10262812e-11],\n",
       "       [9.94888738e-01, 5.11126163e-03, 1.86970715e-11],\n",
       "       [9.69288078e-01, 3.07119203e-02, 1.78128252e-09],\n",
       "       [9.81564582e-01, 1.84354170e-02, 7.20752153e-10],\n",
       "       [9.79899580e-01, 2.01004191e-02, 7.14726493e-10],\n",
       "       [9.83844844e-01, 1.61551561e-02, 3.89967383e-10],\n",
       "       [9.94063551e-01, 5.93644934e-03, 2.22930104e-11],\n",
       "       [9.94024468e-01, 5.97553235e-03, 2.29372690e-11],\n",
       "       [9.85189938e-01, 1.48100618e-02, 4.92937966e-10],\n",
       "       [9.82522973e-01, 1.74770262e-02, 6.59605013e-10],\n",
       "       [9.88161174e-01, 1.18388257e-02, 9.86338432e-11],\n",
       "       [9.97471182e-01, 2.52881791e-03, 1.64376441e-12],\n",
       "       [9.98042661e-01, 1.95733889e-03, 5.69226275e-13],\n",
       "       [9.86172564e-01, 1.38274359e-02, 3.14011984e-10],\n",
       "       [9.93548249e-01, 6.45175122e-03, 3.80025795e-11],\n",
       "       [9.96045888e-01, 3.95411188e-03, 5.07175884e-12],\n",
       "       [9.95438007e-01, 4.56199298e-03, 1.47480405e-11],\n",
       "       [9.87469130e-01, 1.25308693e-02, 5.14988756e-10],\n",
       "       [9.92614843e-01, 7.38515667e-03, 4.55520560e-11],\n",
       "       [9.94311401e-01, 5.68859870e-03, 2.67324491e-11],\n",
       "       [9.40429659e-01, 5.95703133e-02, 2.74861876e-08],\n",
       "       [9.90538765e-01, 9.46123462e-03, 2.32536237e-10],\n",
       "       [9.77013259e-01, 2.29867401e-02, 9.57431830e-10],\n",
       "       [9.86274590e-01, 1.37254097e-02, 2.08954968e-10],\n",
       "       [9.81705349e-01, 1.82946499e-02, 7.58340597e-10],\n",
       "       [9.95672603e-01, 4.32739724e-03, 1.09917218e-11],\n",
       "       [9.90009853e-01, 9.99014666e-03, 2.09880233e-10],\n",
       "       [9.95890837e-01, 4.10916258e-03, 7.31084677e-12],\n",
       "       [9.92528267e-01, 7.47173267e-03, 5.51254345e-11],\n",
       "       [5.57036684e-03, 9.86318501e-01, 8.11113194e-03],\n",
       "       [5.50784342e-03, 9.72410598e-01, 2.20815590e-02],\n",
       "       [2.16196504e-03, 9.50727915e-01, 4.71101202e-02],\n",
       "       [3.76836366e-03, 8.64927719e-01, 1.31303917e-01],\n",
       "       [2.03267621e-03, 9.08506487e-01, 8.94608367e-02],\n",
       "       [3.81225330e-03, 8.79989592e-01, 1.16198155e-01],\n",
       "       [3.30346071e-03, 9.28110285e-01, 6.85862542e-02],\n",
       "       [4.01077904e-02, 9.54461602e-01, 5.43060733e-03],\n",
       "       [4.53312987e-03, 9.75711657e-01, 1.97552127e-02],\n",
       "       [7.60381408e-03, 9.09715970e-01, 8.26802158e-02],\n",
       "       [1.30697577e-02, 9.52460857e-01, 3.44693855e-02],\n",
       "       [6.36770374e-03, 9.55004817e-01, 3.86274793e-02],\n",
       "       [8.32245438e-03, 9.76911802e-01, 1.47657437e-02],\n",
       "       [2.45348117e-03, 8.70166657e-01, 1.27379862e-01],\n",
       "       [2.93358415e-02, 9.66930351e-01, 3.73380774e-03],\n",
       "       [8.04237797e-03, 9.85163616e-01, 6.79400647e-03],\n",
       "       [2.95736682e-03, 8.01761870e-01, 1.95280764e-01],\n",
       "       [1.68358468e-02, 9.76668316e-01, 6.49583725e-03],\n",
       "       [4.55782876e-04, 5.26228832e-01, 4.73315385e-01],\n",
       "       [1.26857757e-02, 9.72434165e-01, 1.48800595e-02],\n",
       "       [6.96923774e-04, 4.96932364e-01, 5.02370712e-01],\n",
       "       [1.18667372e-02, 9.79898575e-01, 8.23468768e-03],\n",
       "       [3.14256395e-04, 4.41011291e-01, 5.58674452e-01],\n",
       "       [3.68532300e-03, 9.30150490e-01, 6.61641875e-02],\n",
       "       [8.22664423e-03, 9.81933364e-01, 9.83999135e-03],\n",
       "       [6.42652761e-03, 9.82097930e-01, 1.14755421e-02],\n",
       "       [1.90287374e-03, 9.36481834e-01, 6.16152921e-02],\n",
       "       [6.30346851e-04, 6.93702665e-01, 3.05666988e-01],\n",
       "       [2.74844961e-03, 8.72936319e-01, 1.24315231e-01],\n",
       "       [4.75575143e-02, 9.51291575e-01, 1.15091052e-03],\n",
       "       [1.26153976e-02, 9.70016180e-01, 1.73684223e-02],\n",
       "       [2.03492379e-02, 9.72543664e-01, 7.10709815e-03],\n",
       "       [1.47435683e-02, 9.76605835e-01, 8.65059709e-03],\n",
       "       [9.52794716e-05, 1.79092466e-01, 8.20812255e-01],\n",
       "       [2.63881064e-03, 7.25510658e-01, 2.71850532e-01],\n",
       "       [5.83244795e-03, 9.49210979e-01, 4.49565726e-02],\n",
       "       [3.15877031e-03, 9.62730437e-01, 3.41107926e-02],\n",
       "       [1.90881214e-03, 8.86831907e-01, 1.11259280e-01],\n",
       "       [1.29587735e-02, 9.69134718e-01, 1.79065088e-02],\n",
       "       [5.82629591e-03, 9.21146940e-01, 7.30267643e-02],\n",
       "       [3.98653223e-03, 8.60746448e-01, 1.35267020e-01],\n",
       "       [3.81837846e-03, 9.29170081e-01, 6.70115402e-02],\n",
       "       [9.85227865e-03, 9.72839085e-01, 1.73086359e-02],\n",
       "       [3.40977400e-02, 9.59777162e-01, 6.12509819e-03],\n",
       "       [5.86155266e-03, 9.27527983e-01, 6.66104648e-02],\n",
       "       [1.40980622e-02, 9.73299395e-01, 1.26025430e-02],\n",
       "       [8.88811430e-03, 9.62023460e-01, 2.90884261e-02],\n",
       "       [7.76352742e-03, 9.77440753e-01, 1.47957199e-02],\n",
       "       [7.16926736e-02, 9.26918287e-01, 1.38903966e-03],\n",
       "       [8.93763594e-03, 9.63575753e-01, 2.74866108e-02],\n",
       "       [2.34765522e-08, 8.37363057e-04, 9.99162613e-01],\n",
       "       [6.82606902e-06, 2.86665261e-02, 9.71326648e-01],\n",
       "       [1.21329092e-06, 2.06379251e-02, 9.79360862e-01],\n",
       "       [7.12287099e-06, 3.95983842e-02, 9.60394493e-01],\n",
       "       [2.62286103e-07, 5.00550622e-03, 9.94994231e-01],\n",
       "       [6.77482528e-08, 4.21665226e-03, 9.95783280e-01],\n",
       "       [4.83461232e-05, 5.91901708e-02, 9.40761483e-01],\n",
       "       [1.20892386e-06, 2.37438707e-02, 9.76254920e-01],\n",
       "       [7.32945011e-07, 1.22180136e-02, 9.87781253e-01],\n",
       "       [4.49502010e-07, 1.04088838e-02, 9.89590667e-01],\n",
       "       [1.39625601e-04, 2.86057119e-01, 7.13803256e-01],\n",
       "       [8.07382473e-06, 4.75678912e-02, 9.52424035e-01],\n",
       "       [5.99424115e-06, 4.86346464e-02, 9.51359359e-01],\n",
       "       [1.61791441e-06, 1.06852279e-02, 9.89313154e-01],\n",
       "       [2.28853791e-07, 2.97533314e-03, 9.97024438e-01],\n",
       "       [4.55611864e-06, 3.03024505e-02, 9.69692993e-01],\n",
       "       [3.03256383e-05, 1.14060036e-01, 8.85909638e-01],\n",
       "       [1.26653530e-06, 2.77382206e-02, 9.72260513e-01],\n",
       "       [4.08225080e-10, 1.59354656e-04, 9.99840645e-01],\n",
       "       [3.17212601e-05, 9.61951471e-02, 9.03773132e-01],\n",
       "       [1.41331245e-06, 1.94318915e-02, 9.80566695e-01],\n",
       "       [1.04150286e-05, 3.27372580e-02, 9.67252327e-01],\n",
       "       [3.77118536e-08, 3.14561076e-03, 9.96854352e-01],\n",
       "       [1.12344969e-04, 2.39254231e-01, 7.60633424e-01],\n",
       "       [7.06792434e-06, 4.83093806e-02, 9.51683551e-01],\n",
       "       [2.20515941e-05, 1.40873678e-01, 8.59104270e-01],\n",
       "       [2.30888351e-04, 3.39744349e-01, 6.60024763e-01],\n",
       "       [2.84521744e-04, 3.51302206e-01, 6.48413273e-01],\n",
       "       [5.20789068e-07, 7.63155303e-03, 9.92367926e-01],\n",
       "       [9.19161314e-05, 3.44087249e-01, 6.55820835e-01],\n",
       "       [1.29407884e-06, 2.70419702e-02, 9.72956736e-01],\n",
       "       [3.92520295e-05, 2.82856552e-01, 7.17104196e-01],\n",
       "       [2.37914141e-07, 4.53643827e-03, 9.95463324e-01],\n",
       "       [4.19740680e-04, 4.92750402e-01, 5.06829857e-01],\n",
       "       [2.20350923e-05, 7.58043011e-02, 9.24173664e-01],\n",
       "       [3.06106553e-07, 1.23677662e-02, 9.87631928e-01],\n",
       "       [8.44912879e-07, 8.97779796e-03, 9.91021357e-01],\n",
       "       [3.99527846e-05, 1.25595181e-01, 8.74364866e-01],\n",
       "       [3.79762027e-04, 3.90361061e-01, 6.09259177e-01],\n",
       "       [2.06268581e-05, 1.14023269e-01, 8.85956104e-01],\n",
       "       [4.44131153e-07, 8.02628082e-03, 9.91973275e-01],\n",
       "       [2.36052109e-05, 1.24819149e-01, 8.75157246e-01],\n",
       "       [6.82606902e-06, 2.86665261e-02, 9.71326648e-01],\n",
       "       [3.63834925e-07, 7.37131979e-03, 9.92628316e-01],\n",
       "       [3.17477258e-07, 6.22052340e-03, 9.93779159e-01],\n",
       "       [5.58865783e-06, 4.36797037e-02, 9.56314708e-01],\n",
       "       [1.27881845e-05, 6.22310823e-02, 9.37756130e-01],\n",
       "       [3.47042280e-05, 1.24858718e-01, 8.75106578e-01],\n",
       "       [4.59652935e-06, 2.58330476e-02, 9.74162356e-01],\n",
       "       [7.67656580e-05, 1.40651397e-01, 8.59271837e-01]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_prob = cls.predict_proba(X)\n",
    "Y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d7a6686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = cls.predict(X)\n",
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bab5e442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(Y, Y_hat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
